{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Set max columns displayed\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "test_id = pd.read_csv('train_identity.csv')\n",
    "test_trans = pd.read_csv('train_transaction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 25.86 Mb (42.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "test_id = reduce_mem_usage(test_id)\n",
    "test_trans = reduce_mem_usage(test_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze train_identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions of data\n",
    "test_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "test_id.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of columns\n",
    "test_id.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data\n",
    "test_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze train_transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trans.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trans.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_trans.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trans.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resumetable(df):\n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n",
    "    summary = summary.reset_index()\n",
    "    summary['Name'] = summary['index']\n",
    "    summary = summary[['Name','dtypes']]\n",
    "    summary['Missing'] = df.isnull().sum().values    \n",
    "    summary['Uniques'] = df.nunique().values\n",
    "    summary['First Value'] = df.loc[0].values\n",
    "    summary['Second Value'] = df.loc[1].values\n",
    "    summary['Third Value'] = df.loc[2].values\n",
    "\n",
    "    for name in summary['Name'].value_counts().index:\n",
    "        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resumetable(test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumetable(test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First goal: We are doing fraud detection, so let us visualize the fraudulent transactions first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trans['TransactionAmt'] = test_trans['TransactionAmt'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trans = len(test_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first step, compare the total transactions vs fraud transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/39922986/pandas-group-by-and-sum\n",
    "total_transaction_amount = test_trans['TransactionAmt'].sum()\n",
    "# https://stackoverflow.com/questions/1823058/how-to-print-number-with-commas-as-thousands-separators\n",
    "f'{round(total_transaction_amount, 2):,}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: It is odd that transactions have so many decimals, it is possible that they come from taxes or other forms of financial transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_transactions = test_trans[test_trans['isFraud'] == 1]['TransactionAmt'].sum()\n",
    "f'{round(fraud_transactions, 2):,}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_fraud_transactions = test_trans[test_trans['isFraud'] == 0]['TransactionAmt'].sum()\n",
    "f'{round(non_fraud_transactions, 2):,}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_transactions + non_fraud_transactions == total_transaction_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_count = test_trans[test_trans['isFraud'] == 1]['TransactionAmt'].count()\n",
    "non_fraud_count = test_trans[test_trans['isFraud'] == 0]['TransactionAmt'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trans == fraud_count + non_fraud_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "# https://www.geeksforgeeks.org/different-ways-to-create-pandas-dataframe/\n",
    "fraud_data = {'Fraudulent':['Fraudulent', 'non-Fraudulent'],\n",
    "              'Transaction Amount':[fraud_transactions, non_fraud_transactions]}\n",
    "fraud_data = pd.DataFrame(fraud_data)\n",
    "# https://seaborn.pydata.org/generated/seaborn.barplot.html\n",
    "# https://stackoverflow.com/questions/42404154/increase-tick-label-font-size-in-seaborn\n",
    "sns.set(style=\"whitegrid\", font_scale=1.5)\n",
    "ax = sns.barplot(x=\"Fraudulent\", y=\"Transaction Amount\", data=fraud_data)\n",
    "ax.set_title('Barplot of Transaction Amount vs. Fraudulent or non-Fraudulent', fontsize=22)\n",
    "ax.set_xlabel('') # Remove the 'Fraudulent' column name from the bottom\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/total_transaction_amount * 100),\n",
    "            ha=\"center\", fontsize=15)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "fraud_count_data = {'Fraudulent':['Fraudulent', 'non-Fraudulent'],\n",
    "              'Fraud Count':[fraud_count, non_fraud_count]}\n",
    "fraud_count_data = pd.DataFrame(fraud_count_data)\n",
    "sns.set(style=\"whitegrid\", font_scale=1.5)\n",
    "ax = sns.barplot(x=\"Fraudulent\", y=\"Fraud Count\", data=fraud_count_data)\n",
    "ax.set_title('Barplot of Transaction Count vs. Fraudulent or non-Fraudulent', fontsize=22)\n",
    "ax.set_xlabel('') # Remove the 'Fraudulent' column name from the bottom\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/total_trans*100),\n",
    "            ha=\"center\", fontsize=15) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the distributions of the transaction amounts for fraudulent vs. non-fraudulent charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fraud_dist = test_trans[test_trans['isFraud'] == 1]['TransactionAmt']\n",
    "non_fraud_dist = test_trans[test_trans['isFraud'] == 0]['TransactionAmt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "ax = sns.distplot(fraud_dist)\n",
    "ax.set_title('Histogram of Fraudulent Transactions Amounts', fontsize=22)\n",
    "ax.set_xlabel('Transaction Amount ($)')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "ax = sns.distplot(non_fraud_dist)\n",
    "ax.set_title('Histogram of non-Fraudulent Transactions Amounts', fontsize=22)\n",
    "ax.set_xlabel('Transaction Amount ($)')\n",
    "ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-examine them both with certain outliers removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_by_iqr(df, column, whisker_width=1.5):\n",
    "#     https://stackoverflow.com/questions/34782063/how-to-use-pandas-filter-with-iqr\n",
    "    \"\"\"Remove outliers from a dataframe by column, including optional \n",
    "       whiskers, removing rows for which the column value are \n",
    "       less than Q1-1.5IQR or greater than Q3+1.5IQR.\n",
    "    Args:\n",
    "        df (`:obj:pd.DataFrame`): A pandas dataframe to subset\n",
    "        column (str): Name of the column to calculate the subset from.\n",
    "        whisker_width (float): Optional, loosen the IQR filter by a\n",
    "                               factor of `whisker_width` * IQR.\n",
    "    Returns:\n",
    "        (`:obj:pd.DataFrame`): Filtered dataframe\n",
    "    \"\"\"\n",
    "    # Calculate Q1, Q2 and IQR\n",
    "    q1 = df[column].quantile(0.25)                 \n",
    "    q3 = df[column].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    # Apply filter with respect to IQR, including optional whiskers\n",
    "    filter = (df[column] >= q1 - whisker_width*iqr) & (df[column] <= q3 + whisker_width*iqr)\n",
    "    return df.loc[filter]                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_dist_outlier = subset_by_iqr(pd.DataFrame(fraud_dist), 'TransactionAmt', whisker_width=1.5)\n",
    "non_fraud_dist_outlier = subset_by_iqr(pd.DataFrame(non_fraud_dist), 'TransactionAmt', whisker_width=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "ax = sns.distplot(fraud_dist_outlier)\n",
    "ax.set_title('Histogram of Fraudulent Transactions Amounts (Outliers Removed)', fontsize=22)\n",
    "ax.set_xlabel('Transaction Amount ($)')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "ax = sns.distplot(non_fraud_dist_outlier)\n",
    "ax.set_title('Histogram of non-Fraudulent Transactions Amounts (Outliers Removed)', fontsize=22)\n",
    "ax.set_xlabel('Transaction Amount ($)')\n",
    "ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-examine with log of original (outliers remain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "ax = sns.distplot(np.log(fraud_dist))\n",
    "ax.set_title('log Histogram of Fraudulent Transactions Amounts', fontsize=22)\n",
    "ax.set_xlabel('Transaction Amount ($)')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "ax = sns.distplot(np.log(non_fraud_dist))\n",
    "ax.set_title('log Histogram of non-Fraudulent Transactions Amounts', fontsize=22)\n",
    "ax.set_xlabel('Transaction Amount ($)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-examine them both with certain outliers removed (log version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "ax = sns.distplot(np.log(fraud_dist_outlier))\n",
    "ax.set_title('log Histogram of Fraudulent Transactions Amounts', fontsize=22)\n",
    "ax.set_xlabel('Transaction Amount ($)')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "ax = sns.distplot(np.log(non_fraud_dist_outlier))\n",
    "ax.set_title('log Histogram of non-Fraudulent Transactions Amounts', fontsize=22)\n",
    "ax.set_xlabel('Transaction Amount ($)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of histogram:\n",
    "- Normal histogram (fraud)\n",
    "    - heavily right skewed\n",
    "- Normal histogram (non-fraud)\n",
    "    - heavily right skewed\n",
    "- log histogram (fraud)\n",
    "    - distribution appears closer to normal\n",
    "- log histogram (non-fraud)\n",
    "    - distribution appears closer to normal\n",
    "- Normal histogram (fraud) (w/o outliers)\n",
    "    - less right skew, observations are bunched towards the lower end\n",
    "- Normal histogram (non-fraud) (w/o outliers)\n",
    "    - less right skew, observations have various peaks\n",
    "- log histogram (fraud) (w/o outliers)\n",
    "    - data appears left skewed, somewhat normal\n",
    "- log histogram (non-fraud) (w/o outliers)\n",
    "    - data appears left skewed, two sharp peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcOutliers(df_num):\n",
    "    whisker_width = 1.5\n",
    "    q1 = df_num.quantile(0.25)                 \n",
    "    q3 = df_num.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    #Calculating the higher and lower cut values\n",
    "    lower, upper = q1 - whisker_width*iqr, q3 + whisker_width*iqr\n",
    "\n",
    "    # creating an array of lower, higher and total outlier values \n",
    "    outliers_lower = [x for x in df_num if x < lower]\n",
    "    outliers_higher = [x for x in df_num if x > upper]\n",
    "    outliers_total = [x for x in df_num if x < lower or x > upper]\n",
    "\n",
    "    # array without outlier values\n",
    "    outliers_removed = [x for x in df_num if x > lower and x < upper]\n",
    "    \n",
    "    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n",
    "    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n",
    "    print('Total outlier observations: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n",
    "    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n",
    "    print(\"Total percentual of Outliers: \", round((len(outliers_total) / len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalcOutliers(test_trans['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem in this project is a binary classification problem and for that reason the primary tool for prediction will be logistic regression. The category of fraudulent vs. non-fraudulent is a categorical variable, where neither fraudulent or non-fraudulent can be considered higher or lower than the other. For that reason, it is nominal rather than ordinal.\n",
    "\n",
    "The data we have is count data, so it will be following a Poisson distribution. Possibly Poisson, not independent?\n",
    "https://www.youtube.com/watch?v=sv_KXSiorFk\n",
    "\n",
    "There seems to be issues with the Poisson assumptions. It is definitely count data, but it is difficult to determine whether these events are occuring in a specific time or space. If it were time we could say from start to end date. If it were space or area we could say North America.\n",
    "\n",
    "Approximately Poisson process? Instead it is possible that the random variable approximately follows a Poisson process and we can try to model it as such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraudulent  | Frequency | Proportion\n",
    "------------- | ------------- | -------------\n",
    "Yes  | 20,663 | 0.03499\n",
    "No  | 569,877 | 0.96501\n",
    "Total | 590,540 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trans == fraud_count + non_fraud_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independence? $P(A \\cap B) = P(A)P(B)$ How to test? Monte Carlo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean and variance should be similar for Poisson regression. Different for sample means/sample variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible sci-kit learn solution, use weights in logistic regression. https://chrisalbon.com/machine_learning/logistic_regression/handling_imbalanced_classes_in_logistic_regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-640014a5f83e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# \"class\" is either fradulent (0) and non-fradudulent (1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mfrad_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mnon_frad_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# assume your data is df, and it's a pd dataframe\n",
    "# \"class\" is either fradulent (0) and non-fradudulent (1)\n",
    "\n",
    "frad_df = df[df['class'] == 0].reset_index(drop=True)\n",
    "non_frad_df = df[df['class'] == 1].reset_index(drop=True)\n",
    "\n",
    "random_sample_idx = np.random.choice(range(frad_df.shape[0]), size=non_frad_df.shape[0], replace=True)\n",
    "frad_sample_df = frad_df.iloc[random_sample_idx]\n",
    "balanced_df = pd.concat([non_frad_df, frad_sample_df]).reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
